# Deep-Learning

# Experiments Details:

Experiment 1: Single Artificial Neuron

Objective: Train a single artificial neuron to perform binary classification.

Dataset: OR gate.

Result: The neuron successfully classifies the inputs: [0, 1, 1, 1].
#


Experiment 2: Single Layer Perceptron (SLP)

Objective: Train an SLP to classify data from an AND gate.

Dataset: AND gate.

Result: The perceptron predicts the outputs: [-1, -1, -1, 1].
#


Experiment 3: Multi-Layer Perceptron (MLP) for XOR Gate

Objective: Solve the XOR problem using a multi-layer perceptron with forward propagation and backpropagation.

Dataset: XOR gate.

Result: The MLP predicts the outputs: [0, 1, 1, 0].
#


Experiment 4: Activation Functions

Objective: Explore the impact of different activation functions (Step, Sigmoid, ReLU, Tanh) on model performance.

Dataset: Breast cancer dataset from Scikit-learn.

Results:
Step Activation Accuracy: 92.98%
Sigmoid Activation Accuracy: 62.28%
ReLU Activation Accuracy: 97.37%
Tanh Activation Accuracy: 92.98%
#


Experiment 5: Forward and Backpropagation

Objective: Implement forward propagation and backpropagation manually for a two-layer neural network.

Dataset: XOR gate.

Result: The network successfully learns the XOR logic.
